{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################     Highly abstract policy class! #######################\n",
    "class Policy(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Nothing is required for to construct an abstract policy class \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_action_value(self):\n",
    "        \"Must be redefined\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "############################################################################\n",
    "\n",
    "\n",
    "###################                Uniform            #######################\n",
    "class Uniform_Policy(Policy):\n",
    "\n",
    "    def get_action_value(self , Q_state ):\n",
    "        \"\"\" Q_values numpy array for a given state  size ( number of actions ) \"\"\"\n",
    "        action = np.random.randint (Q_state.size)\n",
    "        return action , Q_state[action]\n",
    "\n",
    "############################################################################\n",
    "\n",
    "###################                Greedy            #######################\n",
    "class Greedy_Policy(Policy):\n",
    "\n",
    "    \n",
    "    def get_action_value(self , Q_state):\n",
    "        action = Q_state.argmax()\n",
    "        return action , Q_state[action]\n",
    "\n",
    "############################################################################\n",
    "\n",
    "###################             Îµ-Greedy            #######################\n",
    "class Epsilon_Greedy_Policy(Policy):\n",
    "    \n",
    "    def __init__(self , eps=0.1 , decay=1.0 ):  \n",
    "        self.eps      = eps\n",
    "        self.decay = decay\n",
    "                    \n",
    "    def get_action_value(self , Q_state ):\n",
    "        self.eps = self.eps*self.decay\n",
    "        if ( np.random.rand() <= (self.eps) ):\n",
    "            action = np.random.choice ( Q_state.size )\n",
    "        else :\n",
    "            action = Q_state.argmax()\n",
    "    \n",
    "        return action , Q_state[action]\n",
    "    \n",
    "    def set_epsilon(self,eps):\n",
    "        \"\"\"\n",
    "        when we want to perform a cutomized decay manually\n",
    "        \n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_policy ( name=\"Epsilon_Greedy\" , params={ \"eps\":0.1 , \"decay\":1.0 } ) :\n",
    "    \n",
    "    if   ( name==\"Uniform\" ): return Uniform_Policy()\n",
    "    elif ( name==\"Greedy\" ) : return Greedy_Policy()\n",
    "    elif ( name==\"Epsilon_Greedy\" ) : return Epsilon_Greedy_Policy(**params)\n",
    "    else : \n",
    "        raise Exception(\"Unknown policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = make_policy( name=\"Epsilon_Greedy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, in_channels=4, num_actions=2):\n",
    "#         \"\"\"\n",
    "#         DQN\n",
    "#         \"\"\"\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "#         self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "#         self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "#         self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "#         return self.fc5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./algo_imgs/DQN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN ( Experience replay + target Network )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity   = capacity\n",
    "        self.curr_state = torch.Tensor().type(torch.float64)\n",
    "        self.next_state = torch.Tensor().type(torch.float64)\n",
    "        self.action     = torch.Tensor().type(torch.uint8)\n",
    "        self.reward     = torch.Tensor().type(torch.float64)\n",
    "        self.final_state_mask = []\n",
    "        \n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, Phi_S, action, Phi_next_S, reward, done):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        \n",
    "        act = torch.tensor([[action]],dtype=torch.uint8)\n",
    "        \n",
    "        if len(self) < self.capacity:\n",
    "            \n",
    "            self.curr_state = torch.cat( [self.curr_state,Phi_S.view(1,-1)] ,0)\n",
    "            self.next_state = torch.cat( [self.next_state,Phi_next_S.view(1,-1)] ,0)\n",
    "            self.action     = torch.cat([self.action,act], 0)\n",
    "            self.reward     = torch.cat([self.reward,torch.tensor([reward],dtype=torch.float64).view(1,-1)], 0)\n",
    "            self.final_state_mask.append(not done)\n",
    "        else :\n",
    "            self.curr_state[self.position]   = Phi_S\n",
    "            self.next_state[self.position]   = Phi_next_S\n",
    "            self.action[self.position]       = act\n",
    "            self.reward[self.position]       = reward\n",
    "            self.final_state_mask[self.position] = done\n",
    "            \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indexes = np.random.choice(len(self),size=batch_size)\n",
    "        return self.curr_state[indexes], self.action[indexes],\\\n",
    "               self.next_state[indexes], self.reward[indexes],\\\n",
    "               torch.tensor( self.final_state_mask ) [indexes] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.curr_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Q_Estimator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Neural Network Q(state) ==> [ Q[state,action1], Q[state,action2], ... ]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self , in_size=4 , num_actions=2 , hidden_size=[] ):\n",
    "        super(DQN_Q_Estimator, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for x in hidden_size:\n",
    "            self.layers.append(nn.Linear(in_size, x))\n",
    "            in_size = x\n",
    "            self.layers.append(nn.Linear(in_size, num_actions))\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = self.layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, action_space, behavior_policy, memory_capacity=1000, hidden_size=[50],hiddevice=device):\n",
    "        \n",
    "        self.action_space      = action_space\n",
    "        self.replay_memory     = ReplayMemory(memory_capacity)\n",
    "        self.Q_estimator_policy= DQN_Q_Estimator(in_size=4,num_actions=action_space.n,hidden_size=hidden_size)\\\n",
    "                                 .double().to(device)\n",
    "        \n",
    "        # copy from policy net\n",
    "        self.Q_estimator_target= DQN_Q_Estimator(in_size=4,num_actions=action_space.n,hidden_size=hidden_size)\\\n",
    "                                 .double().to(device)\n",
    "        \n",
    "        self.behavior_policy = behavior_policy\n",
    "        \n",
    "        self.update_target_network()\n",
    "        \n",
    "    def act(self, observation):\n",
    "        output = self.Q_estimator_policy(observation.to(device)).detach().cpu().numpy()\n",
    "        action, _= self.behavior_policy.get_action_value(output)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"copy policy network parameters to target network parameters\"\n",
    "        self.Q_estimator_target.load_state_dict(self.Q_estimator_policy.state_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = make_policy ( \"Epsilon_Greedy\" , params={ \"eps\" : EPS } )\n",
    "agent = CartPoleAgent( envm.action_space, behavior_policy=p , memory_capacity=MEM_CAPACITY,\n",
    "                       hidden_size=HIDDEN_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0| nbr_actions: 10| epsilon: 0.20\n",
      "episode 10| nbr_actions: 10| epsilon: 0.20\n",
      "episode 20| nbr_actions: 10| epsilon: 0.20\n",
      "episode 30| nbr_actions: 10| epsilon: 0.19\n",
      "episode 40| nbr_actions: 9| epsilon: 0.19\n",
      "episode 50| nbr_actions: 14| epsilon: 0.19\n",
      "episode 60| nbr_actions: 9| epsilon: 0.19\n",
      "episode 70| nbr_actions: 8| epsilon: 0.19\n",
      "episode 80| nbr_actions: 10| epsilon: 0.18\n",
      "episode 90| nbr_actions: 12| epsilon: 0.18\n",
      "episode 100| nbr_actions: 10| epsilon: 0.18\n",
      "100\n",
      "update\n",
      "episode 110| nbr_actions: 12| epsilon: 0.18\n",
      "episode 120| nbr_actions: 11| epsilon: 0.18\n",
      "episode 130| nbr_actions: 11| epsilon: 0.17\n",
      "episode 140| nbr_actions: 9| epsilon: 0.17\n",
      "episode 150| nbr_actions: 12| epsilon: 0.17\n",
      "episode 160| nbr_actions: 9| epsilon: 0.17\n",
      "episode 170| nbr_actions: 10| epsilon: 0.17\n",
      "episode 180| nbr_actions: 9| epsilon: 0.16\n",
      "episode 190| nbr_actions: 11| epsilon: 0.16\n",
      "episode 200| nbr_actions: 12| epsilon: 0.16\n",
      "episode 210| nbr_actions: 12| epsilon: 0.16\n",
      "episode 220| nbr_actions: 9| epsilon: 0.16\n",
      "episode 230| nbr_actions: 10| epsilon: 0.16\n",
      "episode 240| nbr_actions: 10| epsilon: 0.15\n",
      "episode 250| nbr_actions: 12| epsilon: 0.15\n",
      "episode 260| nbr_actions: 9| epsilon: 0.15\n",
      "episode 270| nbr_actions: 13| epsilon: 0.15\n",
      "episode 280| nbr_actions: 12| epsilon: 0.15\n",
      "episode 290| nbr_actions: 10| epsilon: 0.15\n",
      "episode 300| nbr_actions: 9| epsilon: 0.14\n",
      "episode 310| nbr_actions: 12| epsilon: 0.14\n",
      "episode 320| nbr_actions: 20| epsilon: 0.14\n",
      "episode 330| nbr_actions: 9| epsilon: 0.14\n",
      "episode 340| nbr_actions: 10| epsilon: 0.14\n",
      "episode 350| nbr_actions: 11| epsilon: 0.14\n",
      "episode 360| nbr_actions: 10| epsilon: 0.14\n",
      "episode 370| nbr_actions: 10| epsilon: 0.13\n",
      "episode 380| nbr_actions: 9| epsilon: 0.13\n",
      "episode 390| nbr_actions: 10| epsilon: 0.13\n",
      "episode 400| nbr_actions: 9| epsilon: 0.13\n",
      "episode 410| nbr_actions: 10| epsilon: 0.13\n",
      "episode 420| nbr_actions: 10| epsilon: 0.13\n",
      "episode 430| nbr_actions: 9| epsilon: 0.13\n",
      "episode 440| nbr_actions: 10| epsilon: 0.12\n",
      "episode 450| nbr_actions: 11| epsilon: 0.12\n",
      "episode 460| nbr_actions: 10| epsilon: 0.12\n",
      "episode 470| nbr_actions: 10| epsilon: 0.12\n",
      "episode 480| nbr_actions: 9| epsilon: 0.12\n",
      "episode 490| nbr_actions: 13| epsilon: 0.12\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "cartpole_nbr_actions = []\n",
    "\n",
    "outdir = 'cartpole-v0/CartPol-agent-results'\n",
    "envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.seed(0)\n",
    "\n",
    "#####################  hyper params #####################\n",
    "episode_count = 500\n",
    "env.verbose = True\n",
    "np.random.seed(0)\n",
    "\n",
    "rsum = 0\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPS = 0.2\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 100\n",
    "MEM_CAPACITY  = 1000\n",
    "HIDDEN_SIZE = [128]\n",
    "\n",
    "log_interval = 10\n",
    "#########################################################\n",
    "\n",
    "################## AGENT ################## \n",
    "p = make_policy ( \"Epsilon_Greedy\" , params={ \"eps\" : EPS, \"decay\":0.9999 } )\n",
    "agent = CartPoleAgent( envm.action_space, behavior_policy=p , memory_capacity=MEM_CAPACITY,\n",
    "                       hidden_size=HIDDEN_SIZE )\n",
    "###########################################\n",
    "\n",
    "################# LOSS + optimizer ################\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam ( agent.Q_estimator_policy.parameters() , lr=LR ) \n",
    "###################################################\n",
    "\n",
    "nbr_steps = 0\n",
    "\n",
    "for i in range(episode_count):\n",
    "    obs = envm.reset()\n",
    "    env.verbose = (i % 100 == 0 and i > 0)  # afficher 1 episode sur 100\n",
    "    if env.verbose:\n",
    "        env.render()\n",
    "    j = 0\n",
    "    rsum = 0\n",
    "    \n",
    "    Phi_S = torch.from_numpy(obs)\n",
    "\n",
    "    while True:\n",
    "        action =  agent.act(Phi_S)\n",
    "        j+=1\n",
    "        obs, reward, done, _ = envm.step(action)\n",
    "        Phi_next_S =  torch.from_numpy(obs)\n",
    "        \n",
    "        if done:\n",
    "            reward = -1\n",
    "        # Store the transition in memory\n",
    "        agent.replay_memory.push(Phi_S, action, Phi_next_S, reward, done)\n",
    "        \n",
    "        \n",
    "        if ( len(agent.replay_memory) >= MEM_CAPACITY ):\n",
    "                    \n",
    "            # get X_batch \n",
    "            state_batch, action_batch, next_state_batch, reward_batch, not_final_state_mask = \\\n",
    "            agent.replay_memory.sample(BATCH_SIZE)\n",
    "            \n",
    "\n",
    "            \n",
    "            # create Y_batch\n",
    "            expected_Q = reward_batch.to(device).reshape(-1)\n",
    "            next_Q = agent.Q_estimator_target(next_state_batch.to(device)).max(1)[0]\n",
    "            expected_Q += not_final_state_mask* GAMMA * next_Q\n",
    "            \n",
    "\n",
    "            # Forward pass\n",
    "            current_Q = agent.Q_estimator_policy(state_batch.to(device))\n",
    "            current_Q = current_Q[ list(range(BATCH_SIZE)) , action_batch.reshape(-1).tolist() ]\n",
    "            loss = criterion(current_Q, expected_Q)\n",
    "\n",
    "            # Backward a\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            nbr_steps += 1 \n",
    "         \n",
    "            if( nbr_steps == TARGET_UPDATE ) :\n",
    "                print(nbr_steps)\n",
    "                agent.update_target_network()\n",
    "                print(\"update\")\n",
    "            \n",
    "        if(done):\n",
    "            if i % log_interval == 0:\n",
    "                print('episode {}| nbr_actions: {}| epsilon: {:.2f}'.format(i, j, agent.behavior_policy.eps))\n",
    "            break\n",
    "           \n",
    "\n",
    "            \n",
    "#         # logs\n",
    "#         rsum += reward\n",
    "#         j += 1\n",
    "#         if env.verbose:\n",
    "#             env.render()\n",
    "#         if done:\n",
    "#             cartpole_nbr_actions.append(j)\n",
    "#             print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "#             break\n",
    "        \n",
    "\n",
    "print(\"done\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN_Q_Estimator(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q_estimator_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
