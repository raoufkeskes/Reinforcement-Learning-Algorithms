{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import json\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "import re \n",
    "\n",
    "# np.set_printoptions(linewidth=40)\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding and Exploring openAi Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------  Introduction  ------------------------------------------ \n",
      "possible actions : \n",
      "----------------\n",
      "Discrete(4)\n",
      "\n",
      " after doing action 1 we get : \n",
      "(array([[1, 1, 1, 1, 1, 1],\n",
      "       [1, 0, 0, 0, 3, 1],\n",
      "       [1, 0, 1, 0, 5, 1],\n",
      "       [1, 0, 0, 0, 2, 1],\n",
      "       [1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1]]), 0, False, {})\n",
      "----------------\n",
      "\n",
      " all possible states (statedic) :\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  0\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 2 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  1\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 2 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  2\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 2 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  3\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 2 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  4\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  5\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 2 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  6\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 2 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  7\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 2 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  8\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 2 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  9\n",
      "----------------\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 2 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  10\n",
      "----------------\n",
      "number of States :  11\n",
      "----------------\n",
      "-------------------------------------  Example of a state and its transitions  ------------------------------ \n",
      " state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "----------------\n",
      " \n",
      "\n",
      " its transitions : \n",
      "----------------\n",
      "\n",
      "\n",
      "##########\n",
      "action  0\n",
      "##########\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.8\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 2 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "##########\n",
      "action  1\n",
      "##########\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 2 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.8\n",
      "Reward :  -1\n",
      "Done   :  True\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 2 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "##########\n",
      "action  2\n",
      "##########\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 2 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  -1\n",
      "Done   :  True\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 2 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.8\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "##########\n",
      "action  3\n",
      "##########\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 2 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.1\n",
      "Reward :  -1\n",
      "Done   :  True\n",
      "----------------\n",
      "next state : \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "Proba  :  0.8\n",
      "Reward :  0\n",
      "Done   :  False\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "### create the env\n",
    "env = gym.make(\"gridworld-v0\")\n",
    "\n",
    "# Init the seed  ( pseudo random )\n",
    "env.seed(0) \n",
    "\n",
    "print(\"-----------------------------------------------  Introduction  ------------------------------------------ \")\n",
    "\n",
    "# Print possible actions\n",
    "print(\"possible actions : \")  \n",
    "print(\"----------------\")\n",
    "print(env.action_space)\n",
    "\n",
    "# do action 1\n",
    "# return : Observation(next state) , reward , done ( Boolean : next state is terminal state so the game is finished)\n",
    "print(\"\\n after doing action 1 we get : \")\n",
    "print(env.step(0))  \n",
    "print(\"----------------\")\n",
    "# Visualize the game ( the grid ) \n",
    "# env.render()  \n",
    "# Visualize the grid game in a console mode\n",
    "# env.render(mode=\"human\")\n",
    "\n",
    "# get statedic , MDP\n",
    "statedic, mdp = env.getMDP()\n",
    "# statedic : state -> encoded_number \n",
    "print(\"\\n all possible states (statedic) :\")\n",
    "print(\"----------------\")\n",
    "for state in statedic : \n",
    "    l = ast.literal_eval(state)\n",
    "    print(np.array(l))\n",
    "    print(\"==> \",statedic[state])\n",
    "    print(\"----------------\")\n",
    "print(\"number of States : \",len(statedic))\n",
    "print(\"----------------\")\n",
    "print(\"-------------------------------------  Example of a state and its transitions  ------------------------------ \")\n",
    "#         MDP : dict ( state => transitions from this state )\n",
    "# transitions : dict { action => [ (proba1 , next_state1 , reward1 , done) , (proba2 , next_state2 , reward2 , done) ... ] }\n",
    "state, transitions = list(mdp.items())[0]  # state 0 for example  \n",
    "print(\" state : \" )\n",
    "print(np.array(ast.literal_eval(state)))\n",
    "print(\"----------------\")\n",
    "print(\" \\n\\n its transitions : \")\n",
    "print(\"----------------\\n\\n\")\n",
    "for a in transitions :\n",
    "    print(\"##########\")\n",
    "    print(\"action \",a)\n",
    "    print(\"##########\")\n",
    "    for nuplet in transitions[a] : \n",
    "        print(\"next state : \" )\n",
    "        print(np.array(ast.literal_eval(nuplet[1])))\n",
    "        print(\"Proba  : \" , nuplet[0] )\n",
    "        print(\"Reward : \" , nuplet[2] )\n",
    "        print(\"Done   : \" , nuplet[3] )\n",
    "        print(\"----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminal States are not on the MDP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing from MDP \n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 3 1]\n",
      " [1 0 1 0 2 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  1\n",
      "[[1 1 1 1 1 1]\n",
      " [1 0 0 0 2 1]\n",
      " [1 0 1 0 5 1]\n",
      " [1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n",
      "==>  5\n"
     ]
    }
   ],
   "source": [
    "missingMDP = set(statedic.keys()).difference(set(env.getMDP()[1].keys()))\n",
    "print(\"missing from MDP \")\n",
    "for s in missingMDP : \n",
    "    l = ast.literal_eval(s)\n",
    "    print(np.array(l))\n",
    "    print(\"==> \",statedic[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**they are encoded :**\n",
    "- **Win (Green) : 5**\n",
    "- **Defeat (Red) : 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid World Goal : \n",
    "\n",
    "The Agent **Blue** should harvest **Yellow** elements  and finish on a **Green** case .\n",
    "\n",
    "The agent should avoid **Pink** cases which penalizes , and **Red** ones which are terminal states  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding Mapping** : \n",
    "\n",
    "0 => Empty Case \n",
    "\n",
    "1 => Wall\n",
    "\n",
    "2 => Player ( Our Agent )\n",
    "\n",
    "3 => Green Case\n",
    "\n",
    "4 => Yellow Case\n",
    "\n",
    "5 => Red Case\n",
    "\n",
    "6 => Pink Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different plans  **0 ===> 10** with several difficulty settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "statedic , MDP = env.getMDP()\n",
    "\n",
    "S = len(statedic)\n",
    "A = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_policy ( S , A , init=\"Deterministic\" ) : \n",
    "    \n",
    "    '''\n",
    "    S : number of states \n",
    "    A : number of actions\n",
    "    init : ( 'Deterministic' | 'Random' | 'Uniform' )\n",
    "    '''\n",
    "    if ( init == 'Uniform' ) :\n",
    "        return np.ones((S,A),dtype=np.float16) / A\n",
    "    \n",
    "    p = np.zeros((S,A),dtype=np.float16)\n",
    "    \n",
    "    if ( init == 'Deterministic' ) : \n",
    "        p[ list(range(9)) , np.random.choice ( 4 , size=9 ) ] = 1\n",
    "        return p \n",
    "        \n",
    "    # else random  rounded 1e-1  like [ 0.6 , 0.2 , 0.0 , 0.1 ]\n",
    "    for i in range (S) :\n",
    "        Sum = 0 \n",
    "        for j in range (A) :\n",
    "            if ( j < A-1 ) : \n",
    "                p[i][j] = round( np.random.rand() * ( 1-Sum ) , 1 )\n",
    "                Sum += p[i][j]\n",
    "                if ( Sum >= 1 ):\n",
    "                    break\n",
    "            else :\n",
    "                p[i][j] = (1-Sum)\n",
    "    return np.round(p,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "deter_policy =  gen_policy ( S , A )\n",
    "rand_policy  =  gen_policy ( S , A , 'Random' )\n",
    "uni_policy   =  gen_policy ( S , A , 'Uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deter_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0.2, 0.8],\n",
       "       [0.1, 0.6, 0. , 0.3],\n",
       "       [0.5, 0.1, 0.1, 0.3],\n",
       "       [0.5, 0.5, 0. , 0. ],\n",
       "       [0.8, 0.2, 0. , 0. ],\n",
       "       [0.6, 0.1, 0.2, 0.1],\n",
       "       [0.3, 0.3, 0.3, 0.1],\n",
       "       [0.8, 0.1, 0.1, 0. ],\n",
       "       [0.7, 0. , 0. , 0.3],\n",
       "       [0.3, 0.6, 0. , 0.1],\n",
       "       [0.4, 0.2, 0.3, 0.1]], dtype=float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]], dtype=float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy_eval ( env , Policy , Eps=1e-5 , discount=0.99 , verbose =True ) :\n",
    "    \n",
    "    # init \n",
    "    statedic , MDP = env.getMDP()\n",
    "    V_prev = np.random.rand( env.nS )\n",
    "    actions = np.arange( env.nA )\n",
    "    \n",
    "    #for printing \n",
    "    n_iter_eval = 1\n",
    "    \n",
    "    # loop applying Bellman Operator untill convergence   \n",
    "    while(True) : \n",
    "        V_curr = np.zeros ( V_prev.shape ) \n",
    "        # for each state s_t\n",
    "        for s in MDP :\n",
    "            Sum_a = 0 \n",
    "            # action according to policy\n",
    "            for a in MDP[s] :\n",
    "                Pi_s_a = Policy[ statedic[s] ][a]\n",
    "                if ( Pi_s_a > 0 ) :\n",
    "                    # for each state s_t+1 \n",
    "                    Sum_snext = 0\n",
    "                    for proba, next_state, reward, done in MDP[s][a] :\n",
    "                        Sum_snext +=   (proba * ( reward + discount * V_prev[ statedic[next_state] ] ))\n",
    "                    \n",
    "                    Sum_a += Pi_s_a * Sum_snext\n",
    "                    \n",
    "            V_curr[ statedic[s] ] = Sum_a\n",
    "            \n",
    "        if ( np.linalg.norm( V_curr - V_prev ) < Eps ) :\n",
    "            if ( verbose == True ) : print(\"n_iter_eval for convergence : \",n_iter_eval)\n",
    "            break \n",
    "        \n",
    "        n_iter_eval += 1 \n",
    "        V_prev = V_curr\n",
    "\n",
    "    return V_curr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter_eval for convergence :  108\n"
     ]
    }
   ],
   "source": [
    "V = Policy_eval ( env , deter_policy , Eps=1e-5 , discount=0.99 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.93690001,  0.        , -0.44592837, -0.50296922, -0.32583501,\n",
       "        0.        , -0.27761546, -0.2574881 , -0.1225327 ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Since state 1 and 5 are terminal states there will be no policies for them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy_iteration ( env , Eps=1e-5 , discount=0.99 , init='Uniform' , verbose = False  ) : \n",
    "    \n",
    "    \n",
    "    # init \n",
    "    statedic , MDP = env.getMDP()\n",
    "    P_prev = gen_policy ( S , A , init )\n",
    "    actions = np.arange( env.nA )\n",
    "    \n",
    "    n_iter_policy = 1\n",
    "    while ( True ): \n",
    "        # Policy Evaluation \n",
    "        V = Policy_eval ( env , P_prev , Eps=Eps , discount=discount , verbose=False )\n",
    "        \n",
    "        # Improvement\n",
    "        P_curr = np.zeros ( P_prev.shape )\n",
    "        for s in MDP :\n",
    "            # lookahead\n",
    "            acts = np.zeros( env.nA ) \n",
    "            for a in MDP[s] :\n",
    "                Sum_snext = 0\n",
    "                for proba, next_state, reward, done in MDP[s][a] :\n",
    "                    Sum_snext +=   (proba * ( reward + discount * V[ statedic[next_state] ] ))\n",
    "                acts[a] = Sum_snext\n",
    "            # select the argmax         \n",
    "            P_curr[ statedic[s] ][ acts.argmax() ] = 1.0\n",
    "            \n",
    "        if ( np.array_equal(P_prev,P_curr) ) :\n",
    "            if ( verbose ) : print(\"n_iter_policy for convergence : \",n_iter_policy)\n",
    "            break\n",
    "            \n",
    "        n_iter_policy += 1 \n",
    "        \n",
    "        # Printing\n",
    "        if ( verbose ) : \n",
    "            print(\"--------------------\")\n",
    "            print(P_prev)\n",
    "            print(P_curr)\n",
    "            print(\"--------------------\")\n",
    "        P_prev = P_curr\n",
    "\n",
    "    return P_curr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = Policy_iteration ( env, Eps=1e-5 , discount=0.99   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value_iteration ( env , Eps=1e-5 , discount=0.99 , init='Uniform' , verbose = True  ) :\n",
    "    \n",
    "    # init \n",
    "    statedic , MDP = env.getMDP()\n",
    "    V_prev = np.random.rand( env.nS )\n",
    "    actions = np.arange( env.nA )\n",
    "    \n",
    "    P = np.zeros  ( (env.nS,env.nA) )\n",
    "    \n",
    "    n_iter = 1 \n",
    "    while ( True ) :\n",
    "        \n",
    "        # value iteration \n",
    "        V_curr = np.zeros ( V_prev.shape ) \n",
    "        # for each state s_t\n",
    "        for s in MDP :\n",
    "            # lookahead\n",
    "            acts = np.zeros( env.nA ) \n",
    "            for a in MDP[s] :\n",
    "                Sum_snext = 0\n",
    "                for proba, next_state, reward, done in MDP[s][a] :\n",
    "                    Sum_snext +=   (proba * ( reward + discount * V_prev[ statedic[next_state] ] ))\n",
    "                acts[a] = Sum_snext\n",
    "                    \n",
    "            V_curr[ statedic[s] ] = acts.max()\n",
    "            \n",
    "        if ( np.linalg.norm( V_curr - V_prev ) < Eps ) :\n",
    "            if ( verbose == True ) : print(\"n_iter_value_iteration for convergence : \",n_iter)\n",
    "            break\n",
    "            \n",
    "        n_iter += 1\n",
    "        V_prev = V_curr\n",
    "            \n",
    "    V  = V_curr\n",
    "    # create a policy\n",
    "    for s in MDP :\n",
    "        # lookahead\n",
    "        acts = np.zeros( env.nA ) \n",
    "        for a in MDP[s] :\n",
    "            Sum_snext = 0\n",
    "            for proba, next_state, reward, done in MDP[s][a] :\n",
    "                Sum_snext +=   (proba * ( reward + discount * V[ statedic[next_state] ] ))\n",
    "            acts[a] = Sum_snext\n",
    "        # select the argmax         \n",
    "        P[ statedic[s] ][ acts.argmax() ] = 1.0\n",
    "    \n",
    "    return P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter_value_iteration for convergence :  85\n"
     ]
    }
   ],
   "source": [
    "P2 = Value_iteration ( env, Eps=1e-5 , discount=0.99   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging to the same Determinstic policy  Confirmation =  True\n"
     ]
    }
   ],
   "source": [
    "print ( \"Converging to the same Determinstic policy  Confirmation = \" , np.array_equal ( P1 , P2 ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Method = \"Policy_iteration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "class DeterministicPolicy_Agent(object):\n",
    "    \"\"\"An agent using a deterministic policy generated by Policy Iteration Algorithm !\"\"\"\n",
    "    \n",
    "    def __init__(self, env, Method=\"Policy_iteration\", init=\"Uniform\", discount=0.99 ):\n",
    "        \n",
    "        self.action_space = env.action_space\n",
    "        self.policy       = globals()[Method]( env , Eps=1e-5 , discount=discount , init=init , verbose = False  )\n",
    "                        \n",
    "    def act(self, observation , reward , done ):\n",
    "        return self.policy[observation].argmax()\n",
    "    \n",
    "    \n",
    "class Epsilon_Policy_Agent(object):\n",
    "    \"\"\"An agent using a deterministic policy generated by Policy Iteration Algorithm !\"\"\"\n",
    "    def __init__(self, action_space , env , eps = 0.9 ):\n",
    "        self.action_space = action_space\n",
    "        self.policy       = policy\n",
    "        self.eps          = eps\n",
    "    \n",
    "    def act(self, observation , reward , done ):\n",
    "        if ( np.random.rand() <= self.eps ) :\n",
    "            return self.policy[observation].argmax()\n",
    "        else :\n",
    "            return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3e73fa2549a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mFPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# afficher 1 episode sur 100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         )\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ffmpeg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "# Faire un fichier de log sur plusieurs scenarios\n",
    "outdir = 'gridworld-v0/random-agent-results'\n",
    "envm = wrappers.Monitor(env, directory=outdir, force=True )\n",
    "env.setPlan(\"gridworldPlans/plan0.txt\", {0: -0.1, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "env.seed()  # Initialiser le pseudo aleatoire\n",
    "\n",
    "# agent \n",
    "agent = DeterministicPolicy_Agent( env  )\n",
    "\n",
    "episode_count = 1000\n",
    "reward = 0\n",
    "done = False\n",
    "rsum = 0\n",
    "FPS = 0.0001\n",
    "for i in range(episode_count):\n",
    "    obs = envm.reset()\n",
    "    env.verbose = (i % 100 == 0 and i > 0)  # afficher 1 episode sur 100\n",
    "    if env.verbose:\n",
    "        env.render(FPS)\n",
    "    j = 0\n",
    "    rsum = 0\n",
    "    while True:\n",
    "        # convert state string to its encoding number \n",
    "        encoded_state = statedic[str(obs.tolist())]\n",
    "        action = agent.act( encoded_state , reward , done )\n",
    "        obs, reward, done, _ = envm.step(action)\n",
    "        rsum += reward\n",
    "        j += 1\n",
    "        if env.verbose:\n",
    "            env.render(FPS)\n",
    "        if done:\n",
    "            print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "            break\n",
    "\n",
    "print(\"done\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
